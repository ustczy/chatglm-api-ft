{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08778553",
   "metadata": {},
   "source": [
    "### 安装 git lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7dc84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs && git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b8a58",
   "metadata": {},
   "source": [
    "### 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38b85e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/THUDM/chatglm-6b ../chatglm-6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd3f6d",
   "metadata": {},
   "source": [
    "### 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065819a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r ./requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742f6cd-6a69-4cb3-995b-667887e3965e",
   "metadata": {},
   "source": [
    "### 数据集准备 \n",
    "\n",
    "准备 `.jsonl` 格式的数据放到 ./data 目录下。数据格式为：\n",
    "\n",
    "\n",
    "{\"q\": \"问题\", \"a\": \"回答\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45107640",
   "metadata": {},
   "source": [
    "### 对数据集进行分词\n",
    "\n",
    "为了避免每次训练的时都要重新对数据集分词，先分好词形成特征后保存成可直接用于训练的数据集。相关参数说明：\n",
    "\n",
    "* model_checkpoint: 模型目录\n",
    "* input_file:  ./data 目录下的数据集文件名\n",
    "* prompt_key:  数据集中 prompt 对应的字段（这里是 q）\n",
    "* target_key:  数据集中 completion 对应的字段（这里是 a)\n",
    "* save_name:  数据集保存目录，分词后的数据保存在 ./data/tokenized_data 下\n",
    "* max_seq_length:  文本最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4698e-5fb8-41fb-917b-a344b11b12d8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python ./script/tokenize_dataset_rows.py \\\n",
    "    --input_file dataset.jsonl \\\n",
    "    --prompt_key q \\\n",
    "    --target_key a \\\n",
    "    --save_name dataset \\\n",
    "    --max_seq_length 2000 \\\n",
    "    --skip_overlength False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80056f78-037a-4611-a3ac-5fcb6413d18f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 使用 LoRA 微调\n",
    "\n",
    "参数说明：\n",
    "\n",
    "* tokenized_dataset: 分词后的数据集保存目录（即上一步 save_name 的值）\n",
    "* tlora_rank: 设置 LoRA 的秩，推荐为4或8，显存够的话使用8\n",
    "* tper_device_train_batch_size: 每块 GPU 上的 batch size\n",
    "* tgradient_accumulation_steps: 梯度累加，可以在不提升显存占用的情况下增大 batch size\n",
    "* tmax_steps: 训练步数\n",
    "* tsave_steps: 多少步保存一次\n",
    "* tsave_total_limit: 保存多少个checkpoint\n",
    "* tlogging_steps: 多少步打印一次训练情况(loss, lr, etc.)\n",
    "* toutput_dir: 模型文件保存地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577a8fb0-e08b-415e-8a14-14b4e1e30940",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-07-17T11:41:04.744402Z",
     "iopub.status.busy": "2023-07-17T11:41:04.744053Z",
     "iopub.status.idle": "2023-07-17T11:57:42.944714Z",
     "shell.execute_reply": "2023-07-17T11:57:42.943956Z",
     "shell.execute_reply.started": "2023-07-17T11:41:04.744382Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "'\\nlen(dataset)=1199\\n'\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:07<00:00,  1.07it/s]\n",
      "{'': 0}\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "/home/pai/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                  | 0/2000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "{'loss': 2.9936, 'learning_rate': 4.8875e-06, 'epoch': 0.08}                    \n",
      "{'loss': 2.2516, 'learning_rate': 4.7625000000000006e-06, 'epoch': 0.17}        \n",
      "{'loss': 1.8654, 'learning_rate': 4.6375e-06, 'epoch': 0.25}                    \n",
      "{'loss': 1.3975, 'learning_rate': 4.5125e-06, 'epoch': 0.33}                    \n",
      "{'loss': 1.0253, 'learning_rate': 4.3875e-06, 'epoch': 0.42}                    \n",
      "{'loss': 0.7799, 'learning_rate': 4.2625e-06, 'epoch': 0.5}                     \n",
      "{'loss': 0.4884, 'learning_rate': 4.137500000000001e-06, 'epoch': 0.58}         \n",
      "{'loss': 0.4856, 'learning_rate': 4.0125e-06, 'epoch': 0.67}                    \n",
      "{'loss': 0.3893, 'learning_rate': 3.8875000000000005e-06, 'epoch': 0.75}        \n",
      "{'loss': 0.3898, 'learning_rate': 3.7625e-06, 'epoch': 0.83}                    \n",
      "{'loss': 0.2899, 'learning_rate': 3.6375000000000003e-06, 'epoch': 0.92}        \n",
      "{'loss': 0.2815, 'learning_rate': 3.5125000000000003e-06, 'epoch': 1.0}         \n",
      "{'loss': 0.2017, 'learning_rate': 3.3875e-06, 'epoch': 1.08}                    \n",
      "{'loss': 0.1721, 'learning_rate': 3.2625e-06, 'epoch': 1.17}                    \n",
      "{'loss': 0.2397, 'learning_rate': 3.1375e-06, 'epoch': 1.25}                    \n",
      "{'loss': 0.1872, 'learning_rate': 3.0125000000000004e-06, 'epoch': 1.33}        \n",
      "{'loss': 0.219, 'learning_rate': 2.8875000000000003e-06, 'epoch': 1.42}         \n",
      "{'loss': 0.1548, 'learning_rate': 2.7625000000000002e-06, 'epoch': 1.5}         \n",
      "{'loss': 0.167, 'learning_rate': 2.6375e-06, 'epoch': 1.58}                     \n",
      "{'loss': 0.2215, 'learning_rate': 2.5125e-06, 'epoch': 1.67}                    \n",
      "{'loss': 0.193, 'learning_rate': 2.3875e-06, 'epoch': 1.75}                     \n",
      "{'loss': 0.1247, 'learning_rate': 2.2625000000000004e-06, 'epoch': 1.83}        \n",
      "{'loss': 0.1337, 'learning_rate': 2.1375000000000003e-06, 'epoch': 1.92}        \n",
      "{'loss': 0.1861, 'learning_rate': 2.0125000000000002e-06, 'epoch': 2.0}         \n",
      "{'loss': 0.1474, 'learning_rate': 1.8875000000000001e-06, 'epoch': 2.09}        \n",
      "{'loss': 0.1265, 'learning_rate': 1.7625e-06, 'epoch': 2.17}                    \n",
      "{'loss': 0.1465, 'learning_rate': 1.6375000000000002e-06, 'epoch': 2.25}        \n",
      "{'loss': 0.1869, 'learning_rate': 1.5125000000000001e-06, 'epoch': 2.34}        \n",
      "{'loss': 0.0842, 'learning_rate': 1.3875000000000003e-06, 'epoch': 2.42}        \n",
      "{'loss': 0.1171, 'learning_rate': 1.2625000000000002e-06, 'epoch': 2.5}         \n",
      "{'loss': 0.2023, 'learning_rate': 1.1375000000000001e-06, 'epoch': 2.59}        \n",
      "{'loss': 0.1233, 'learning_rate': 1.0125e-06, 'epoch': 2.67}                    \n",
      "{'loss': 0.1077, 'learning_rate': 8.875000000000001e-07, 'epoch': 2.75}         \n",
      "{'loss': 0.1315, 'learning_rate': 7.625e-07, 'epoch': 2.84}                     \n",
      "{'loss': 0.1429, 'learning_rate': 6.375e-07, 'epoch': 2.92}                     \n",
      "{'loss': 0.1324, 'learning_rate': 5.125e-07, 'epoch': 3.0}                      \n",
      "{'loss': 0.1154, 'learning_rate': 3.8750000000000005e-07, 'epoch': 3.09}        \n",
      "{'loss': 0.1009, 'learning_rate': 2.6250000000000003e-07, 'epoch': 3.17}        \n",
      "{'loss': 0.1422, 'learning_rate': 1.375e-07, 'epoch': 3.25}                     \n",
      "{'loss': 0.1069, 'learning_rate': 1.2500000000000001e-08, 'epoch': 3.34}        \n",
      "{'train_runtime': 967.5902, 'train_samples_per_second': 4.134, 'train_steps_per_second': 2.067, 'train_loss': 0.42380734157562255, 'epoch': 3.34}\n",
      "100%|███████████████████████████████████████| 2000/2000 [16:07<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# 删除上次的微调模型\n",
    "# !rm -rf /mnt/workspace/glm-fine-tuning/weights\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python ./script/chatglm_lora_tuning.py \\\n",
    "    --tokenized_dataset dataset \\\n",
    "    --lora_rank 8 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --max_steps 2000 \\\n",
    "    --save_steps 200 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --learning_rate 5e-6 \\\n",
    "    --fp16 \\\n",
    "    --remove_unused_columns false \\\n",
    "    --logging_steps 50 \\\n",
    "    --output_dir ./weights/api-fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef793c13-d40a-44eb-ae51-600a45fdf0b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 加载微调模型\n",
    "\n",
    "微调模型保存在上一步配置的 output_dir 目录下。至少需要其中的 adapter_model.bin、adapter_config.json 两个文件才能部署成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d8b3f-ad12-4258-8ff6-7360216ff978",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 启动 web 服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b20ab-1054-4e7e-abe7-c4db08d1b8c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-17T11:58:11.456121Z",
     "iopub.status.busy": "2023-07-17T11:58:11.455520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:07<00:00,  1.02it/s]\n",
      "/mnt/workspace/glm-api-ft/./server/web.py:92: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10).style(\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/routes.py\", line 439, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/blocks.py\", line 1384, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/blocks.py\", line 1103, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/utils.py\", line 343, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/utils.py\", line 336, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/home/pai/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/pai/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/pai/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/utils.py\", line 319, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/home/pai/lib/python3.9/site-packages/gradio/utils.py\", line 688, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/mnt/workspace/glm-api-ft/./server/web.py\", line 70, in predict\n",
      "    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,\n",
      "  File \"/home/pai/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py\", line 1311, in stream_chat\n",
      "    for outputs in self.stream_generate(**inputs, **gen_kwargs):\n",
      "  File \"/home/pai/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py\", line 1381, in stream_generate\n",
      "    logits_warper = self._get_logits_warper(generation_config)\n",
      "  File \"/home/pai/lib/python3.9/site-packages/transformers/generation/utils.py\", line 768, in _get_logits_warper\n",
      "    warpers.append(TemperatureLogitsWarper(generation_config.temperature))\n",
      "  File \"/home/pai/lib/python3.9/site-packages/transformers/generation/logits_process.py\", line 177, in __init__\n",
      "    raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n",
      "ValueError: `temperature` has to be a strictly positive float, but is 0\n"
     ]
    }
   ],
   "source": [
    "!python ./server/web.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5b141-eec7-4bee-9d99-cf8562bdba6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 通过 API 服务测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c3e3e4-4b16-4913-a512-b5f093951982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T08:50:59.798885Z",
     "iopub.status.busy": "2023-07-13T08:50:59.798543Z",
     "iopub.status.idle": "2023-07-13T08:51:11.552372Z",
     "shell.execute_reply": "2023-07-13T08:51:11.551799Z",
     "shell.execute_reply.started": "2023-07-13T08:50:59.798861Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25h/etc/dsw/node/bin/lt -> /etc/dsw/node/lib/node_modules/localtunnel/bin/lt.jsming\u001b[0m \u001b[35maction:finalize\u001b[0m\u001b[0m\u001b[K\n",
      "+ localtunnel@2.0.2\n",
      "added 22 packages from 22 contributors in 10.96s\n"
     ]
    }
   ],
   "source": [
    "# 安装 pyngrok 用来暴露服务\n",
    "\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a1f0e-d66a-484c-ab38-66517f1103df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!lt --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea84dc-0364-419c-b135-0262ae051a52",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 后台运行 chatlm\n",
    "get_ipython().system_raw(\"python ./server/api.py &\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
